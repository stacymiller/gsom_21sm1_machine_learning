{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Task\" data-toc-modified-id=\"Task-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Task</a></span><ul class=\"toc-item\"><li><span><a href=\"#Binary-encoding-and-metrics\" data-toc-modified-id=\"Binary-encoding-and-metrics-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Binary encoding and metrics</a></span></li></ul></li><li><span><a href=\"#Naive-baseline\" data-toc-modified-id=\"Naive-baseline-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Naive baseline</a></span><ul class=\"toc-item\"><li><span><a href=\"#Metrics\" data-toc-modified-id=\"Metrics-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Metrics</a></span></li></ul></li><li><span><a href=\"#Model-that-can-read\" data-toc-modified-id=\"Model-that-can-read-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model that can read</a></span></li><li><span><a href=\"#GridSearch-for-model-parameters\" data-toc-modified-id=\"GridSearch-for-model-parameters-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>GridSearch for model parameters</a></span><ul class=\"toc-item\"><li><span><a href=\"#Manual-search\" data-toc-modified-id=\"Manual-search-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Manual search</a></span></li><li><span><a href=\"#Automated-search\" data-toc-modified-id=\"Automated-search-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Automated search</a></span></li></ul></li><li><span><a href=\"#Decision-trees\" data-toc-modified-id=\"Decision-trees-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Decision trees</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_palette('muted')\n",
    "sns.set_color_codes('muted')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Seminar plan:\n",
    "\n",
    "1. Grid Search & friends\n",
    "1. Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtrack.jetbrains.com/issues/IDEA\n",
    "\n",
    "Predict the issue type at the moment when the new issue is reported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process model:\n",
    "\n",
    "1. External users create a new issue. They specify its summary and description. Author ID and creation date are recorded automatically. For simplicity, we think that summary and description cannot be changed since then.\n",
    "1. At some point in time issue becomes resolved. We're interested in the value of the Priority field at this moment. Again, for simplicity we suppose that the value of the Priority field did not change since then.\n",
    "\n",
    "Therefore everything we need is `id`, `reporter`, `created`, `summary` and `description` of all resolved IDEA issues that were created by an external user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/issues.json.zip', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code in this cell is less readable, more usable analog of:\n",
    "# df['type'] = df.customFields.map(lambda x: [cf['value']['name'] for cf in x if cf['name'] == 'Type'][0])\n",
    "# df['reporter'] = df.reporter.map(lambda x: x['login'])\n",
    "# df['created'] = pd.to_datetime(df.created, unit='ms')\n",
    "\n",
    "formatter = DataFrameMapper([\n",
    "    ('customFields', FunctionTransformer(\n",
    "        lambda c: c.map(lambda x: [cf['value']['name'] for cf in x if cf['name'] == 'Type'][0])\n",
    "    ), {'alias': 'type'}),\n",
    "    ('reporter', FunctionTransformer(lambda c: c.map(lambda x: x['login']))),\n",
    "    ('created', FunctionTransformer(lambda c: pd.to_datetime(c, unit='ms'))),\n",
    "    ('summary', None),\n",
    "    ('description', None),\n",
    "    (['summary', 'description'], \n",
    "     FunctionTransformer(lambda x: x.summary.fillna('') + '\\n\\n' + x.description.fillna('')),\n",
    "     dict(alias='text')\n",
    "    ),\n",
    "    ('idReadable', None)\n",
    "], input_df=True, df_out=True)\n",
    "formatter.fit_transform(df).sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = formatter.transform(df)[['idReadable', 'summary', 'description', 'text', 'reporter', 'created']]\n",
    "X.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = formatter.transform(df)['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary encoding and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to encode the target to binary: `y_binary = y == 'Bug'` and `y_binary = y != 'Bug'`. Which one to choose?\n",
    "\n",
    "It depends on which errors are more critical to us and which metrics do we use. \n",
    "\n",
    "Example: it is more important to decrease the load on support engineers (who handle bugs) $\\implies$ we need to detect as many non-bugs as possible $\\implies$ we have to choose `y_binary = y != 'Bug'` and look closely at the recall rate (percent of all non-bugs that were discovered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_transformer = FunctionTransformer(lambda c: c != 'Bug')\n",
    "y_binary = binary_transformer.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binary.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_bin_train, y_bin_test = train_test_split(X, y_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.fit(X_train, y_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.predict(X_train)# .any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true=y_bin_train, y_pred=dummy.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(estimator=dummy, X=X_train, y_true=y_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_true=y_bin_train, y_pred=dummy.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_true=y_bin_train, y_pred=dummy.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model that can read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = DataFrameMapper([\n",
    "    ('text', TfidfVectorizer(\n",
    "            min_df=.05, max_df=.5, token_pattern=r'[A-Za-z]{2,}', stop_words='english'\n",
    "    ))\n",
    "], input_df=True, df_out=True).fit(X_train)\n",
    "preprocessor.transform(X_train.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = make_pipeline(preprocessor.set_params(df_out=False), LogisticRegression())\n",
    "lr.fit(X_train, y_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(X_train).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true=y_bin_train, y_pred=lr.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(estimator=lr, X=X_train, y_true=y_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_true=y_bin_train, y_pred=lr.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_true=y_bin_train, y_pred=lr.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true=y_bin_train, y_pred=lr.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(min_df=[.05, .1], max_df=[.2, .3, .5])\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: code the grid search =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(params)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)\n",
    "results\n",
    "# results.drop(columns='estimator').sort_values('test_recall').style.bar(vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short reminder: precision and recall are threshold-dependent, it is better to use sth elsee for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = results.loc[results.test_recall.idxmax()].estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_bin_test, best_estimator.predict_proba(X_test['text'])[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_bin_test, best_estimator.predict(X_test['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(y=precision[:-1], x=recall[:-1], text=thresholds, labels=dict(x='recall', y='precision'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(\n",
    "        TfidfVectorizer(token_pattern=r'[A-Za-z]{2,}', stop_words='english'),\n",
    "        LogisticRegression(penalty='none')\n",
    "    )\n",
    "cv = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=dict(min_df=[.05, .02], max_df=[.1, .3, .6]),\n",
    "    scoring=,\n",
    "    refit=False,\n",
    "    verbose=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(X_train['text'], y_bin_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to speed up:\n",
    "\n",
    "- `GridSearchCV(n_jobs=-1)` would parallel the fitting process\n",
    "- smaller sample would decrease the fit time\n",
    "- smaller number of parameters (greedy strategy) would allow to fit less models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cv.cv_results_)[['params', 'mean_test_precision', 'mean_test_recall']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = make_pipeline(\n",
    "        TfidfVectorizer(token_pattern=r'[A-Za-z]{2,}', stop_words='english', max_df=.3, min_df=.2),\n",
    "        DecisionTreeClassifier()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X_train['text'], y_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(estimator=dt, X=X_train['text'], y_true=y_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_true=y_bin_train, y_pred=dt.predict(X_train['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_true=y_bin_train, y_pred=dt.predict(X_train['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precision_score(y_true=y_bin_test, y_pred=dt.predict(X_test['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_true=y_bin_test, y_pred=dt.predict(X_test['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above is simply overfitted. WHat should we do with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.named_steps['decisiontreeclassifier'].get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = make_pipeline(\n",
    "        TfidfVectorizer(token_pattern=r'[A-Za-z]{2,}', stop_words='english', max_df=.3, min_df=.2),\n",
    "        DecisionTreeClassifier(max_depth=40, min_samples_leaf=10)\n",
    "    )\n",
    "dt.fit(X_train['text'], y_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(estimator=dt, X=X_train['text'], y_true=y_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_true=y_bin_train, y_pred=dt.predict(X_train['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_true=y_bin_train, y_pred=dt.predict(X_train['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: run grid search to find the best parameters for the Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
