{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Task\" data-toc-modified-id=\"Task-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Task</a></span></li><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#Naive-baseline\" data-toc-modified-id=\"Naive-baseline-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Naive baseline</a></span><ul class=\"toc-item\"><li><span><a href=\"#Metrics\" data-toc-modified-id=\"Metrics-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Metrics</a></span></li></ul></li><li><span><a href=\"#Simple-model\" data-toc-modified-id=\"Simple-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Simple model</a></span></li><li><span><a href=\"#Model-that-can-read\" data-toc-modified-id=\"Model-that-can-read-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model that can read</a></span></li><li><span><a href=\"#GridSearch-for-model-parameters\" data-toc-modified-id=\"GridSearch-for-model-parameters-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>GridSearch for model parameters</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_palette('muted')\n",
    "sns.set_color_codes('muted')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Seminar plan:\n",
    "1. Underfitting and overfitting.\n",
    "2. Quality metrics (precision/recall/F1/...)\n",
    "3. Grid Search & friends\n",
    "4. (probably) Time-aware model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtrack.jetbrains.com/issues/IDEA\n",
    "\n",
    "Predict the issue type at the moment when the new issue is reported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process model:\n",
    "\n",
    "1. External users create a new issue. They specify its summary and description. Author ID and creation date are recorded automatically. For simplicity, we think that summary and description cannot be changed since then.\n",
    "1. At some point in time issue becomes resolved. We're interested in the value of the Priority field at this moment. Again, for simplicity we suppose that the value of the Priority field did not change since then.\n",
    "\n",
    "Therefore everything we need is `id`, `reporter`, `created`, `summary` and `description` of all resolved IDEA issues that were created by an external user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Scraped from https://youtrack.jetbrains.com/issues/IDEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/issues.json.zip', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code in this cell is less readable, more usable analog of:\n",
    "# df['type'] = df.customFields.map(lambda x: [cf['value']['name'] for cf in x if cf['name'] == 'Type'][0])\n",
    "# df['reporter'] = df.reporter.map(lambda x: x['login'])\n",
    "# df['created'] = pd.to_datetime(df.created, unit='ms')\n",
    "\n",
    "formatter = DataFrameMapper([\n",
    "    ('customFields', FunctionTransformer(\n",
    "        lambda c: c.map(lambda x: [cf['value']['name'] for cf in x if cf['name'] == 'Type'][0])\n",
    "    ), {'alias': 'type'}),\n",
    "    ('reporter', FunctionTransformer(lambda c: c.map(lambda x: x['login']))),\n",
    "    ('created', FunctionTransformer(lambda c: pd.to_datetime(c, unit='ms'))),\n",
    "    ('summary', None),\n",
    "    ('description', None),\n",
    "    (['summary', 'description'], \n",
    "     FunctionTransformer(lambda x: x.summary.fillna('') + '\\n\\n' + x.description.fillna('')),\n",
    "     dict(alias='text')\n",
    "    ),\n",
    "    ('idReadable', None)\n",
    "], input_df=True, df_out=True)\n",
    "formatter.fit_transform(df).sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = formatter.transform(df)[['idReadable', 'summary', 'description', 'text', 'reporter', 'created']]\n",
    "X.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = formatter.transform(df)['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: which type of the classifier do we need to build, binary or multiclass?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive baseline id a good place to **settle all your evaluation procedures**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_bin_train, y_bin_test = train_test_split(X, y_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.fit(X_train, y_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.predict(X_train)# .any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: How does the confusion matrix for the DummyClassifier look like?\n",
    "\n",
    "**Q**: What are accuracy, precision and recall values for the DummyClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=X_train.created.dt.day_name(), y=y_bin_train, order=list(calendar.day_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a little bit lower probability to file a bug on Sunday. Maybe it can be encoded in a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "preprocessor = DataFrameMapper([\n",
    "    ('created', make_pipeline(\n",
    "        FunctionTransformer(lambda d: d.dt.day_name().to_frame()),\n",
    "        OneHotEncoder()\n",
    "    ))\n",
    "], input_df=True, df_out=True)\n",
    "preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = make_pipeline(preprocessor, LogisticRegression())\n",
    "lr.fit(X_train, y_bin_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: what is the quality of the day-of-week model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model that can read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have texts. How can we transform texts to a set of features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = DataFrameMapper([\n",
    "    ('text', TfidfVectorizer(\n",
    "            min_df=.05, max_df=.5, token_pattern=r'[A-Za-z]{2,}', stop_words='english'\n",
    "    ))\n",
    "], input_df=True, df_out=True).fit(X_train)\n",
    "preprocessor.transform(X_train.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = make_pipeline(preprocessor.set_params(df_out=False), LogisticRegression())\n",
    "lr.fit(X_train, y_bin_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Evaluate the model quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we interested in how many actual bugs we identified as bugs? Or is it more important not to load the prioritization engine with unrelated stuff? Or, maybe, the ultimate goal is not to miss any suggestion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first time we will write the grid search logic manually. Usually you can employ `GridSearchCV` to do it for you. Or probably you cannot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(min_df=[.01, .1], max_df=[.2, .3, .5])\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for params in ParameterGrid(param_grid):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).sort_values('test_recall').style.bar(vmin=0, vmax=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
